# 总结

## 解决的问题：

- 对于一个模型的问题回答质量进行评价
  - 如果使用人工：花费人力大
  - 如果使用模型：需要确定评价的效果

此作品测试LLM(GPT4)在“评价LLM的回答质量”任务中的表现

在这里，“评价者”是GPT4或人类，

“评价”指的是评价者对一个LLM回答的质量给出的评价，

“表现”指的是评价者评价的质量，是该评价接近人类评价的程度

## 主要内容：

通过人类专家打分和群众打分两项实验，展示了GPT4在评价模型答案任务中超越人类的表现

## 实验：

实验包括两个角色，**回答者**和**评价者**。其中，我们关心的是**评价者**能否像人类一样能够准确地判断出**回答者**答案的优劣。

**回答者**任务：回答**一个**或**两个**问题

**评价者**评价方式：对于回答者的答案**打分**或**择优**

**打分**指的是**评价者**会被给予一个**回答者**的回答，并给出打分

**择优**指的是**评价者**会被给予两个**回答者**的回答，选出其中更好的那一个

**评价者**的评价与人类接近的程度会被度量，并作为它的表现好坏的评判标准，是实验最终需要测量的数据

## 实验发现：

GPT4的表现**与人类评价者相比**，展示了GPT4与人类评价者在“给出的评价是否接近人类评价”这项指标上表现接近。

具体来说：

**同意**指的是：两个评价者对同一个择优评价任务（A和B答案相比）给出了同一个评价（A优，相同，B优）。

一个评价者的**与人类同意率**指的是：评价者对所有问题给出的评价，与（其他）人类的评价相比，同意的数量/总数

**与人类接近的程度**指的是与**人类同意率**。在这一项指标上，GPT4在一些情况超越了人类评价者的平均水平。这一项指标根据提问数量的不同（一问，两问）和评价方式的不同（择优，打分）会发生变化。在**两问**，**择优**的情况下，GPT4的表现是85%，人类的平均表现是82%。

### 弱点和改进

相比于人类评价者，LLM评价者仍然具有以下不足：

位置偏见，冗长偏见，自我膨胀偏见，和数学逻辑问题

#### 位置偏见：

交换位置或采用更多数据

#### 冗长偏见：

重复攻击，使用GPT4生成无额外信息的额外文本，比较是否存在额外信息时评价模型的打分

#### 自我膨胀偏见：

由于无法在不改变回答质量的前提下改变模型语言风格，因此难以深入研究改进方法。有些模型

#### 数学逻辑问题：

思维链提示（CoT）没有用，模型依然会被误导；可以独立提问，并将答案作为评价参考

# 评论

模型能否评价模型一直是我很好奇的问题。如果一个模型已经生成了一个答案，他怎么会认为这个答案不好呢？此前这个问题一直让我无从下手。在小模型中可以输出confidence level，但confidence level无效的时候，就没有办法了。这篇文章用大模型去评价大模型，并且取得了不错的效果，也是解开了我许久以来的疑问。

然而，发布时间接近的[另一项工作](https://huggingface.co/blog/llm-leaderboard#related-work)却也提出了一些不同的意见，认为GPT4与人类评价者的评价相似度根据任务种类不同只有0.5上下，不算是非常的有效。这项数据和本文其实基本符合：在去掉择优评价结果为平手的情况下，本文的0.85同意率折合相似度是0.7，而加上平手的情况相似度会降低。如果这样看，似乎GPT4的表现就不是那么完美了。但无论是用哪一种统计方式，GPT4的表现都会比人类更像人，这才是对我来说最有趣的发现。